{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b29b9e",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Why do we call it a 'regression' when this is actually a classifier? The idea is that the regression doesnt output a class label, but probabilities indicating how likely the output has a label, and is good at **binary classification** based on features.\n",
    "\n",
    "In the case of an image of some animal, which could either be a cat or not a cat. Say: $cat = 1, !cat = 0$.\n",
    "\n",
    "Logistic regression works on binary classification, and produces a value $p\\in [0,1]$\n",
    "\n",
    "---\n",
    "\n",
    "Say we have $n$ features in a feature vector $\\mathbf{X}$, so each feature is $X_j^{(i)}$, and we have $m$ instances.\n",
    "\n",
    "We take these $\\mathbf{X}^{(i)}$ and a parameter matrix (weights and biases, ordinary linear regression) $\\Theta\\rightarrow \\Theta^T\\mathbf{X}^{(i)}=\\mathbf{Z}^{(i)}$, a set of outputs - the logits.\n",
    "\n",
    "For each feature, we have $n$ parameters. This is implied through linear algebra. Our goal is to find $\\Theta$ using gradient descent and optimisation.\n",
    "\n",
    "---\n",
    "\n",
    "We need to find a function with a codomain of $f(x)\\in(0,1)$. This is the sigmoid function.\n",
    "\n",
    "$\\phi(z) = \\frac 1{1 + e^{-z}}$\n",
    "\n",
    "This allows us to get to this:\n",
    "\n",
    "$h_\\Theta ^{(i)}\\left( \\mathbf{X}^{(i)} \\right) = \\phi\\left(\\mathbf{Z}^{(i)}\\right)$\n",
    "\n",
    "Where $h$ is the estimated probability. We need a function we can optimise now, and we think about this through a **likelihood** approach.\n",
    "\n",
    "$\\mathcal{L}\\left(\\Theta\\right)=\\prod_{i=1}^m \\left[ h_\\Theta ^{(i)}\\left( \\mathbf{X}^{(i)} \\right) \\right]^{y^{(i)}}\\left[ 1-h_\\Theta ^{(i)}\\left( \\mathbf{X}^{(i)} \\right) \\right]^{1-y^{(i)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b77f5e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999\n"
     ]
    }
   ],
   "source": [
    "# say that the sigmoid function produced 0.999, and it is correct (ie. y=1)\n",
    "\n",
    "print(0.999**1 * (1-0.999)**0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9925ab97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010000000000000009\n"
     ]
    }
   ],
   "source": [
    "# what about if sigmoid produces 0.999 but y=0?\n",
    "\n",
    "print(0.999**0 * (1-0.999)**1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3bac8",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "If we get things right, we get numbers close to the sigmoid, and if we are wrong, the likelihood gets reduced.\n",
    "\n",
    "There are a few faults though.\n",
    "1. we need to negate the likelihood so that we can perform gradient descent, we want a loss function. So, we are going to minimise $\\mathcal{L}^{-1}$\n",
    "2. its numerically unstable, so we want to take $\\ln$ too. $\\rightarrow \\ln{\\mathcal{L}}$\n",
    "3. We also want to divide by the number of instances, since logging the likelihood will turn it into a sum. $\\rightarrow \\frac1m\\times\\cdots$\n",
    "\n",
    "$\\Longrightarrow \\ln{\\mathcal{L}}=\\sum_{i=1}^m {y^{(i)}\\ln{h_\\Theta (\\mathbf{x}^{(i)})} + (1-y^{(i)})\\ln{\\left(1-h_\\Theta (\\mathbf{x}^{(i)})\\right)}}$ is the log-likelihood. Applying the next two...\n",
    "\n",
    "$\n",
    "\\Longrightarrow \\mathcal{J}(\\Theta)=-\\frac1m \\sum_{i=1}^m \\left[{y^{(i)}\\ln{h_\\Theta (\\mathbf{x}^{(i)})} + (1-y^{(i)})\\ln{\\left(1-h_\\Theta (\\mathbf{x}^{(i)})\\right)} }\\right]\n",
    "$\n",
    "\n",
    "This is called the \"cross-entropy **loss function**\". It's the negative average log likelihood, and it is also what we want to minimise when performing gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "We have our parameters, $\\Theta$. For each $\\Theta_j$, how much does it have to change for the answer to be correct? How do we adjust the parameters to minimise the loss function?\n",
    "\n",
    "We have $n$ features, so $n$ parameters, one for each feature.\n",
    "\n",
    "$\n",
    "\\frac{\\partial (\\ln{\\mathcal{L}})}{\\partial \\Theta_j} = \\frac{\\partial (\\ln{\\mathcal{L}})}{\\partial h}\\frac{\\partial h}{\\partial z}\\frac{\\partial z}{\\partial \\Theta_j}\n",
    "$\n",
    "\n",
    "The individual factors can be simplified, and was done on paper.\n",
    "\n",
    "$\n",
    "\\longrightarrow \\frac{\\partial (\\ln{\\mathcal{L}})}{\\partial \\Theta_j} = \\left[ \\frac yh - \\frac{1-y}{1-h} \\right]\\left[ h(1-h) \\right]\\mathbf{x}_j^{(i)} = (y-h)\\mathbf{x}_j^{(i)}\n",
    "$\n",
    "\n",
    "$\n",
    "\\Longrightarrow \\frac{\\partial \\mathcal{J}}{\\partial \\Theta_j} = -\\frac{1}m (y^{(i)}-h)\\mathbf{x}_j^{(i)}\n",
    "$\n",
    "\n",
    "We are interested in the vector form of this.\n",
    "\n",
    "$\n",
    "\\nabla_\\Theta \\mathcal{J}(\\Theta) = \\frac1m \\mathbf{X}^T\\left( \\mathbf{y}-h(\\Theta X)\\right)\n",
    "$\n",
    "\n",
    "With a learning rate $\\alpha$, we take $\\Theta\\rightarrow\\Theta-\\alpha\\cdot\\nabla_\\Theta \\mathcal{J}(\\Theta)$ in order to minimise our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1669bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the important functions\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z) -> float: # the sigmoid function\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def gradient(Theta, X, y) -> np.ndarray: # calculate the gradient for gradient descent\n",
    "    return (X.T @ (sigmoid(X @ Theta) - y)) / y.size\n",
    "\n",
    "def gradient_descent(X, y, alpha=0.1, num_iterations=200, tol=1e-7) -> np.ndarray:\n",
    "    # appending a column of ones for each feature\n",
    "    # note: this is, in effect, the \"intercept\" bias, much like in the linear regression algorithm\n",
    "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    Theta = np.zeros(X_bias.shape[1])\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        grad = gradient(Theta, X_bias, y)\n",
    "        Theta -= alpha * grad\n",
    "\n",
    "        # check for tolerance\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            print(\"Tolerance hit, stopping gradient descent\")\n",
    "            break\n",
    "\n",
    "    return Theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49040638",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Create functions to predict based on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b6a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(X, Theta): # no need to specify type, already in the sigmoid function\n",
    "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "    return sigmoid(X_bias @ Theta)\n",
    "\n",
    "def predict(X, Theta, threshold=0.5): # here, type is asserted in the \"astype\" method\n",
    "    return (predict_probs(X, Theta) >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cd9671",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Use scikit-learn's breast cancer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da468c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9846153846153847 \n",
      " Testing accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score # just a simple evaluation\n",
    "\n",
    "# Random seed for reproducibility\n",
    "R = 260725 # date today is 26/7/25, just a fun random seed\n",
    "\n",
    "# simple preprocessing, the data is nice\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=R)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# scale the data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "Theta_hat = gradient_descent(X_train, y_train, alpha=0.1, num_iterations=400)\n",
    "\n",
    "y_pred_train = predict(X_train, Theta_hat)\n",
    "y_pred_test = predict(X_test, Theta_hat)\n",
    "\n",
    "training_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "testing_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Training accuracy:\", training_accuracy, \"\\n\", \"Testing accuracy:\", testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e30d72",
   "metadata": {},
   "source": [
    "Some very promising results. Here is where the code-along ends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
